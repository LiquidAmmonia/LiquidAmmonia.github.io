<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yean  Cheng</title>
    <meta name="author" content="Yean  Cheng" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.gif"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://liquidammonia.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Yean</span>  Cheng
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/cya2024cut-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/cya2024cut-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/cya2024cut-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/cya2024cut.png"
      class="img-fluid z-depth-1 rounded-circle"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="cya2024cut.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

            <div class="address">
              <p>chengyean1999[at]163.com</p> <p>Haidian, Beijing, China</p>

            </div>
          </div>

          <div class="clearfix">
            <p>I am a research engineer at <a href="https://www.zhipuai.cn">Zhipu.AI</a>, focusing on improving the performance of VLM’s on video, alignment, reasoning, etc. My research interets lies visual understanding and generation, language modeling and world modeling.</p>

<p>I received my Master’s degree from the School of Computer Science at Peking University, at <a href="https://ci.idm.pku.edu.cn/">CILab</a> &amp; <a href="http://aiic.pku.edu.cn/">AIIC</a>, advised by Prof. Boxin Shi and Mr. Ming Lei. I received my Bachelor of Engineering degree in Automation and Bachelor of Arts degree in Economics from Tsinghua University in 2021. My academic research topic involves 3D modeling with neural implicit representations, computational photography, and image quality enhancement.</p>

<p>Deep learning is a useful tool (arguably more useful than most people think) for real-world applications. I enjoy tackling various tasks (e.g., molecule design, quantitative trading, interior design, recommendation system, image quality enhancement) with AI techniques and seeing the real-world impact of my work. I have worked (mostly interned) at wonderful AI start-ups (<a href="https://collov.ai/">Collov</a>, <a href="https://quanmol.com/">QuanMol</a>), corporate companies (Alibaba, ByteDance), and a quantitative investment firm (<a href="https://zding.fund/">Definitive Capital Managment</a>). Along the way, I have met many mentors and peers in the field of intelligence. Please feel free to contact me if interested.</p>

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>
          <div class="news">
            <div class="table-responsive" >
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Mar 3, 2024</th>
                  <td>
                    Our paper “MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models” is accepted by CVPR 2025.


                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 22, 2024</th>
                  <td>
                    I will join <a href="https://www.zhipuai.cn">Zhipu.AI</a> as a Research Engineer.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Dec 9, 2023</th>
                  <td>
                    Our paper “Colorizing Monochromatic Radiance Fields” is accepted by The 38th AAAI Conference on Artificial Intelligence and selected for oral presentation.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 21, 2023</th>
                  <td>
                    Our paper “SPLiT: Single Portrait Lighting Estimation Via a Tetrad of Face Intrinsics” is accepted by T-PAMI.

                  </td>
                </tr>
              </table>
            </div>
          </div>


          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/motionbench-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/motionbench-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/motionbench-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/motionbench.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="motionbench.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="hong2025motionbench" class="col-sm-8">
        <!-- Title -->
        <div class="title">MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models</div>
        <!-- Author -->
        <div class="author">
        

        Hong Wenyi*,&nbsp;Cheng Yean*,&nbsp;Yang Zhuoyi*, and
          <span
              class="more-authors"
              title="click to view 6 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Wang Weihan, Wang Lefan, Gu Xiaotao, Huang Shiyu, Dong Yuxiao, Tang Jie' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >6 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>CVPR</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/https://arxiv.org/abs/2501.02955" class="btn btn-sm z-depth-0" role="button">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://motion-bench.github.io/" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/THUDM/MotionBench" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.48550/arXiv.2501.02955"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.48550/arXiv.2501.02955"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models’ motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM’s ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hong2025motionbench</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wenyi*, Hong and Yean*, Cheng and Zhuoyi*, Yang and Weihan, Wang and Lefan, Wang and Xiaotao, Gu and Shiyu, Huang and Yuxiao, Dong and Jie, Tang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2501.02955}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/cogvideox-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/cogvideox-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/cogvideox-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cogvideox.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="cogvideox.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="yang2025cogvideox" class="col-sm-8">
        <!-- Title -->
        <div class="title">CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer</div>
        <!-- Author -->
        <div class="author">
        

        Zhuoyi Yang,&nbsp;Jiayan Teng,&nbsp;Wendi Zheng, and
          <span
              class="more-authors"
              title="click to view 8 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '8 more authors' ? 'Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng,  others' : '8 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >8 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>ICLR</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/https://arxiv.org/abs/2408.06072" class="btn btn-sm z-depth-0" role="button">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://github.com/THUDM/CogVideo" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/THUDM/CogVideo" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.48550/arXiv.2408.06072"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.48550/arXiv.2408.06072"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos that align seamlessly with text prompts, with a frame rate of 16 fps and resolution of 768 x 1360 pixels. Previous video generation models often struggled with limited motion and short durations. It is especially difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we introduce a 3D Variational Autoencoder (VAE) to compress videos across spatial and temporal dimensions, enhancing both the compression rate and video fidelity. Second, to improve text-video alignment, we propose an expert transformer with expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing progressive training and multi-resolution frame packing, CogVideoX excels at generating coherent, long-duration videos with diverse shapes and dynamic movements. In addition, we develop an effective pipeline that includes various pre-processing strategies for text and video data. Our innovative video captioning model significantly improves generation quality and semantic alignment. Results show that CogVideoX achieves state-of-the-art performance in both automated benchmarks and human evaluation.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2025cogvideox</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2408.06072}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/cogvlm2-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/cogvlm2-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/cogvlm2-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cogvlm2.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="cogvlm2.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="hong2024cogvlm2" class="col-sm-8">
        <!-- Title -->
        <div class="title">CogVLM2: Visual Language Models for Image and Video Understanding</div>
        <!-- Author -->
        <div class="author">
        

        Wenyi Hong,&nbsp;Weihan Wang,&nbsp;Ming Ding, and
          <span
              class="more-authors"
              title="click to view 8 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '8 more authors' ? 'Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue,  others' : '8 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >8 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Technical Report</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/https://arxiv.org/abs/2408.16500" class="btn btn-sm z-depth-0" role="button">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://github.com/THUDM/CogVLM2" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/THUDM/CogVLM2" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.48550/arXiv.2408.16500"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.48550/arXiv.2408.16500"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to 1344×1344 pixels. As a video understanding model, CogVLM2-Video integrates multi-frame input with timestamps and proposes automated temporal grounding data construction. Notably, CogVLM2 family has achieved state-of-the-art results on benchmarks like MMBench, MM-Vet, TextVQA, MVBench and VCGBench.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hong2024cogvlm2</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Technical Report}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CogVLM2: Visual Language Models for Image and Video Understanding}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2408.16500}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/colornerf-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/colornerf-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/colornerf-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/colornerf.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="colornerf.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="cheng2024colornerf" class="col-sm-8">
        <!-- Title -->
        <div class="title">\textcolorblue[Oral] Colorizing Monochromatic Radiance Fields</div>
        <!-- Author -->
        <div class="author">
        

        <em>Yean Cheng</em>,&nbsp;Renjie Wan,&nbsp;Shuchen Weng, and
          <span
              class="more-authors"
              title="click to view 3 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Chengxuan Zhu, Yakun Chang, Boxin Shi' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >3 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>AAAI</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/http://arxiv.org/abs/2402.12184" class="btn btn-sm z-depth-0" role="button">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27895" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/LiquidAmmonia/colornerf" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.1609/aaai.v38i2.27895"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.1609/aaai.v38i2.27895"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cheng2024colornerf</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cheng, Yean and Wan, Renjie and Weng, Shuchen and Zhu, Chengxuan and Chang, Yakun and Shi, Boxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1317-1325}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{\textcolor{blue}{[Oral] Colorizing Monochromatic Radiance Fields}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v38i2.27895}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/split-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/split-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/split-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/split.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="split.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="10301699" class="col-sm-8">
        <!-- Title -->
        <div class="title">SPLiT: Single Portrait Lighting Estimation Via a Tetrad of Face Intrinsics</div>
        <!-- Author -->
        <div class="author">
        

        Fei Fan*,&nbsp;<em>Yean Cheng*</em>,&nbsp;Yongjie Zhu, and
          <span
              class="more-authors"
              title="click to view 4 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Qian Zheng, Si Li, Gang Pan, Boxin Shi' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >4 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE T-PAMI</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/10301699" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/costrice/split" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.1109/TPAMI.2023.3328453"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.1109/TPAMI.2023.3328453"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a novel pipeline to estimate a non-parametric environment map with high dynamic range from a single human face image. Lighting-independent and -dependent intrinsic images of the face are first estimated separately in a cascaded network. The influence of face geometry on the two lighting-dependent intrinsics, diffuse shading and specular reflection, are further eliminated by distributing the intrinsics pixel-wise onto spherical representations using the surface normal as indices. This results in two representations simulating images of a diffuse sphere and a glossy sphere under the input scene lighting. Taking into account the distinctive nature of light sources and ambient terms, we further introduce a two-stage lighting estimator to predict both accurate and realistic lighting from these two representations. Our model is trained supervisedly on a large-scale and high-quality synthetic face image dataset. We demonstrate that our method allows accurate and detailed lighting estimation and intrinsic decomposition, outperforming state-of-the-art methods both qualitatively and quantitatively on real face images.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10301699</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fan*, Fei and Cheng*, Yean and Zhu, Yongjie and Zheng, Qian and Li, Si and Pan, Gang and Shi, Boxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE T-PAMI}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SPLiT: Single Portrait Lighting Estimation Via a Tetrad of Face Intrinsics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TPAMI.2023.3328453}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/GAT_GRU_MASSIVE-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/GAT_GRU_MASSIVE-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/GAT_GRU_MASSIVE-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/GAT_GRU_MASSIVE.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="GAT_GRU_MASSIVE.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="10374148" class="col-sm-8">
        <!-- Title -->
        <div class="title">Fault Diagnosis of Energy Networks Based on Improved Spatial–Temporal Graph Neural Network With Massive Missing Data</div>
        <!-- Author -->
        <div class="author">
        

        Jingfei Zhang,&nbsp;<em>Yean Cheng</em>,&nbsp;and&nbsp;Xiao He</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Automation Science and Engineering</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10374148" class="btn btn-sm z-depth-0" role="button">HTML</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.1109/TASE.2023.3281394"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.1109/TASE.2023.3281394"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In order to ensure the safe and reliable operation of the energy system, real-time fault diagnosis technology is indispensable. Energy systems are typically complex systems consisting of multiple subsystems that are coupled with each other. Before and after the occurrence of a fault, the system is generally in an abnormal or even harsh environment, which may cause a large number of randomly missing measurement data and make the application of fault diagnosis technology extremely difficult. In this paper, the graph attention network (GAT) is improved by a Gaussian mixture model (GMM) for incomplete-data representation. The iteratively updated expectation of the GMM serves as the characterization of missing data, which significantly improves the ability to fill in missing data. The GAT fuses multi-source data according to the topology structure so as to comprehensively exploit the spatial information. The gated recurrent units (GRU) extract dynamic fault information from embedded spatial features and classify the time series into various fault types. Moreover, we propose a loss function in the form of weighted focal loss so that the fault-class imbalance issue brought by the data deficiency can be solved. The proposed uniform spatial-temporal graph neural network classification framework together with the GMM (GM-STGNN) can effectively improve fault diagnosis performance and is applied on an experimental platform of an authentic industrial estate. Results of comparative experiments under different conditions of both sufficient and deficient data illustrate the efficiency and advancement of the proposed method. Note to Practitioners —This paper presents a fault diagnosis method for large-scale energy systems with massive missing data. The proposed GM-STGNN framework can be applied in complex energy networks consisting of coupling subsystems, such as power grids, heating networks, and gas networks. With an incomplete-data representation mechanism, the proposed method utilizes topology information to comprehensively exploit spatial features, it also recurrently transmits historical embedded features and extracts dynamic fault characteristics. Therefore, it can effectively improve energy-network fault identification accuracy when more than half of the sample exists vacant values randomly. In the training procedure, after pre-setting the model scale, data acquired by multi-source sensors is put into the model according to the real topology structure, and corresponding fault labels serve as the supervision. The statistical characteristics of missing data are learned with neural-network parameters until the loss converges. In practical application, the sampling data is divided by a time window of a few seconds. The missing data is mitigated by the estimated expectation of the GMM. Therefore, real-time fault classification results can be obtained with high accuracy. The effectiveness of the proposed method is illustrated by fault diagnosis of a typical distributed heating network under the noise influence. Benefiting from the ability to learn fault knowledge, the proposed method can be easily applied to new scenarios where the process data and topology structure of the system are known.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10374148</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Jingfei and Cheng, Yean and He, Xiao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Automation Science and Engineering}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fault Diagnosis of Energy Networks Based on Improved Spatial–Temporal Graph Neural Network With Massive Missing Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-12}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TASE.2023.3281394}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/GAT_GRU-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/GAT_GRU-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/GAT_GRU-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/GAT_GRU.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="GAT_GRU.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="9927491" class="col-sm-8">
        <!-- Title -->
        <div class="title">Fault Diagnosis of Energy Networks: A Graph Embedding Learning Approach</div>
        <!-- Author -->
        <div class="author">
        

        Jingfei Zhang,&nbsp;<em>Yean Cheng</em>,&nbsp;and&nbsp;Xiao He</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Instrumentation and Measurement</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/9927491" class="btn btn-sm z-depth-0" role="button">HTML</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.1109/TIM.2022.3216669"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.1109/TIM.2022.3216669"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>For industrial parks containing energy systems, fault diagnosis technology is of great significance for their safe operation. In recent years, the topology of energy systems has become more complex due to the use of technologies such as cogeneration, leading to multienergy coupling. Critical equipment and user nodes in these complex energy networks are vulnerable to a lack of sensor data or non-idealities in the measurement environment. There is an urgent need for a unified and robust fault-diagnosis framework for the overall system to identify faults even under non-ideal data conditions. In this article, to address the problem of fault identification and state prediction, a novel deep learning model is constructed based on graph-embedded recurrent neural networks (RNNs) with self-attentional layers. Unstructured data are put into the graph neural network to extract common spatial features. An additive attention mechanism is implemented in the graph attention network (GAT) to integrate multiscale node information. The graph operator is computed within a gated recurrent unit (GRU) that captures the full range of temporal features. In addition, loss functions are introduced for fault identification and state prediction. Data from an industrial park experiment platform are used for fault identification experiments. The advantages of the proposed approach are illustrated by comparative experiments with different levels of missing data.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9927491</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Jingfei and Cheng, Yean and He, Xiao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Instrumentation and Measurement}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fault Diagnosis of Energy Networks: A Graph Embedding Learning Approach}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{71}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-11}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TIM.2022.3216669}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/assets/img/publication_preview/SPSR-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/assets/img/publication_preview/SPSR-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/assets/img/publication_preview/SPSR-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/SPSR.png"
      class="preview z-depth-1 rounded"  
      width="auto" 
      height="auto" 
       
       
       
       
      alt="SPSR.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/cvpr/MaRCCL020" class="col-sm-8">
        <!-- Title -->
        <div class="title">Structure-Preserving Super Resolution With Gradient Guidance</div>
        <!-- Author -->
        <div class="author">
        

        Cheng Ma,&nbsp;Yongming Rao,&nbsp;<em>Yean Cheng</em>, and
          <span
              class="more-authors"
              title="click to view 3 more authors"
              onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Ce Chen, Jiwen Lu, Jie Zhou' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              "
          >3 more authors</span></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In CVPR</em>, 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/9156994" class="btn btn-sm z-depth-0" role="button">HTML</a>
            <a href="https://github.com/Maclory/SPSR" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"
              
                data-doi="10.1109/CVPR42600.2020.00779"
              
              ></span>
              <span class="__dimensions_badge_embed__"
              
                data-doi="10.1109/CVPR42600.2020.00779"
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversarial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image-space loss functions, the gradient-space objectives help generative networks concentrate more on geometric structures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR methods. Visual results demonstrate our superiority in restoring structures while generating natural SR images.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/cvpr/MaRCCL020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Cheng and Rao, Yongming and Cheng, Yean and Chen, Ce and Lu, Jiwen and Zhou, Jie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Structure-Preserving Super Resolution With Gradient Guidance}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7766--7775}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Computer Vision Foundation / {IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPR42600.2020.00779}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Tue, 31 Aug 2021 14:00:04 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/cvpr/MaRCCL020.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%63%79%61%31%37@%73%74%75.%70%6B%75.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=SG9ntS0AAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/LiquidAmmonia" title="GitHub"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/yean-cheng-0595a61a3" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2025 Yean  Cheng. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
Last updated: March 03, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
